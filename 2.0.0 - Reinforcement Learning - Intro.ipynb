{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\def\\CC{\\bf C}\n",
    "\\def\\QQ{\\bf Q}\n",
    "\\def\\RR{\\bf R}\n",
    "\\def\\ZZ{\\bf Z}\n",
    "\\def\\NN{\\bf N}\n",
    "$$\n",
    "# RL Intro\n",
    "\n",
    "In a nutshell, RL is the study of agents and how they learn by trial and error. It formalizes the idea that rewarding or punishing an agent for its behavior makes it more likely to repeat or forego that behavior in the future.\n",
    "\n",
    "## What Can RL Do?\n",
    "\n",
    "RL methods have recently enjoyed a wide variety of successes. For example, it's been used to teach computers to control robots in simulation...\n",
    "\n",
    "<video autoplay=\"\" src=\"https://d4mucfpksywv.cloudfront.net/openai-baselines-ppo/knocked-over-stand-up.mp4\" loop=\"\" controls=\"\" style=\"display: block; margin-left: auto; margin-right: auto; margin-bottom:1.5em; width: 100%; max-width: 720px; max-height: 80vh;\">\n",
    "</video>\n",
    "\n",
    "## Key Concepts and Terminology\n",
    "\n",
    "![Agent-environment interaction loop.](images/rl_diagram_transparent_bg.png)\n",
    "\n",
    "The main characters of RL are the **agent** and the **environment**. The environment is the world that the agent lives in and interacts with. At every step of interaction, the agent sees a (possibly partial) observation of the state of the world, and then decides on an action to take. The environment changes when the agent acts on it, but may also change on its own.\n",
    "\n",
    "The agent also perceives a **reward** signal from the environment, a number that tells it how good or bad the current world state is. The goal of the agent is to maximize its cumulative reward, called **return**. Reinforcement learning methods are ways that the agent can learn behaviors to achieve its goal.\n",
    "\n",
    "\n",
    "### States and Observations\n",
    "\n",
    "A **state** $s$ is a complete description of the state of the world. There is no information about the world which is hidden from the state. An **observation** $o$ is a partial description of a state, which may omit information.\n",
    "\n",
    "In deep RL, we almost always represent states and observations by a [real-valued vector, matrix, or higher-order tensor](). For instance, a visual observation could be represented by the RGB matrix of its pixel values; the state of a robot might be represented by its joint angles and velocities.\n",
    "\n",
    "When the agent is able to observe the complete state of the environment, we say that the environment is **fully observed**. When the agent can only see a partial observation, we say that the environment is **partially observed**.\n",
    "\n",
    "\n",
    "### Action Spaces\n",
    "\n",
    "Different environments allow different kinds of actions. The set of all valid actions in a given environment is often called the **action space**. Some environments, like Atari and Go, have **discrete action spaces**, where only a finite number of moves are available to the agent. Other environments, like where the agent controls a robot in a physical world, have **continuous action spaces**. In continuous spaces, actions are real-valued vectors.\n",
    "\n",
    "This distinction has some quite-profound consequences for methods in deep RL. Some families of algorithms can only be directly applied in one case, and would have to be substantially reworked for the other.\n",
    "\n",
    "### Policies\n",
    "\n",
    "A **policy** is a rule used by an agent to decide what actions to take. It can be deterministic, in which case it is usually denoted by $\\mu$ :\n",
    "\n",
    "$$a_t = \\mu(s_t),$$\n",
    "\n",
    "or it may be stochastic, in which case it is usually denoted by $\\pi$ :\n",
    "\n",
    "$$a_t \\sim \\pi(\\cdot | s_t).$$\n",
    "\n",
    "Because the policy is essentially the agent's brain, it's not uncommon to substitute the word \"policy\" for \"agent\", eg saying \"The policy is trying to maximize reward.\"\n",
    "\n",
    "In deep RL, we deal with **parameterized policies**: policies whose outputs are computable functions that depend on a set of parameters (eg the weights and biases of a neural network) which we can adjust to change the behavior via some optimization algorithm.\n",
    "\n",
    "We often denote the parameters of such a policy by $\\theta$ or $\\phi$, and then write this as a subscript on the policy symbol to highlight the connection:\n",
    "\n",
    "$$a_t = \\mu_{\\theta}(s_t)$$\n",
    "\n",
    "$$a_t \\sim \\pi_{\\theta}(\\cdot | s_t).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Stochastic Policies\n",
    "\n",
    "The two most common kinds of stochastic policies in deep RL are **categorical policies** and **diagonal Gaussian policies**.\n",
    "\n",
    "[Categorical]() policies can be used in discrete action spaces, while diagonal [Gaussian]() policies are used in continuous action spaces.\n",
    "\n",
    "Two key computations are centrally important for using and training stochastic policies:\n",
    "\n",
    "-   sampling actions from the policy,\n",
    "-   and computing log likelihoods of particular actions, $\\log \\pi_{\\theta}(a|s)$.\n",
    "\n",
    "In what follows, we'll describe how to do these for both categorical and diagonal Gaussian policies.\n",
    "\n",
    "Categorical Policies\n",
    "\n",
    "A categorical policy is like a classifier over discrete actions. You build the neural network for a categorical policy the same way you would for a classifier: the input is the observation, followed by some number of layers (possibly convolutional or densely-connected, depending on the kind of input), and then you have one final linear layer that gives you logits for each action, followed by a [softmax]() to convert the logits into probabilities.\n",
    "\n",
    "**Sampling.** Given the probabilities for each action, frameworks like Tensorflow have built-in tools for sampling. For example, see the [tf.distributions.Categorical]() documentation, or [tf.multinomial]().\n",
    "\n",
    "**Log-Likelihood.** Denote the last layer of probabilities as $P_{\\theta}(s)$. It is a vector with however many entries as there are actions, so we can treat the actions as indices for the vector. The log likelihood for an action $a$ can then be obtained by indexing into the vector:\n",
    "\n",
    "$$\\log \\pi_{\\theta}(a|s) = \\log \\left[P_{\\theta}(s)\\right]_a.$$\n",
    "\n",
    "Diagonal Gaussian Policies\n",
    "\n",
    "A multivariate Gaussian distribution (or multivariate normal distribution, if you prefer) is described by a mean vector, $\\mu$, and a covariance matrix, $\\Sigma$. A diagonal Gaussian distribution is a special case where the covariance matrix only has entries on the diagonal. As a result, we can represent it by a vector.\n",
    "\n",
    "A diagonal Gaussian policy always has a neural network that maps from observations to mean actions, $\\mu_{\\theta}(s)$. There are two different ways that the covariance matrix is typically represented.\n",
    "\n",
    "**The first way:** There is a single vector of log standard deviations, $\\log \\sigma$, which is **not** a function of state: the $\\log \\sigma$ are standalone parameters. (You Should Know: our implementations of VPG, TRPO, and PPO do it this way.)\n",
    "\n",
    "**The second way:** There is a neural network that maps from states to log standard deviations, $\\log \\sigma_{\\theta}(s)$. It may optionally share some layers with the mean network.\n",
    "\n",
    "Note that in both cases we output log standard deviations instead of standard deviations directly. This is because log stds are free to take on any values in $(-\\infty, \\infty)$, while stds must be nonnegative. It's easier to train parameters if you don't have to enforce those kinds of constraints. The standard deviations can be obtained immediately from the log standard deviations by exponentiating them, so we do not lose anything by representing them this way.\n",
    "\n",
    "**Sampling.** Given the mean action $\\mu_{\\theta}(s)$ and standard deviation $\\sigma_{\\theta}(s)$, and a vector $z$ of noise from a spherical Gaussian ($z \\sim \\mathcal{N}(0, I)$), an action sample can be computed with\n",
    "\n",
    "$$a = \\mu_{\\theta}(s) + \\sigma_{\\theta}(s) \\odot z,$$\n",
    "\n",
    "where $\\odot$ denotes the elementwise product of two vectors. Standard frameworks have built-in ways to compute the noise vectors, such as [tf.random\\_normal](). Alternatively, you can just provide the mean and standard deviation directly to a [tf.distributions.Normal]() object and use that to sample.\n",
    "\n",
    "**Log-Likelihood.** The log-likelihood of a $k$ -dimensional action $a$, for a diagonal Gaussian with mean $\\mu = \\mu_{\\theta}(s)$ and standard deviation $\\sigma = \\sigma_{\\theta}(s)$, is given by\n",
    "\n",
    "$$\\log \\pi_{\\theta}(a|s) = -\\frac{1}{2}\\left(\\sum_{i=1}^k \\left(\\frac{(a_i - \\mu_i)^2}{\\sigma_i^2} + 2 \\log \\sigma_i \\right) + k \\log 2\\pi \\right).$$\n",
    "\n",
    "### Trajectories\n",
    "\n",
    "A trajectory $\\tau$ is a sequence of states and actions in the world,\n",
    "\n",
    "$$\\tau = (s_0, a_0, s_1, a_1, ...).$$\n",
    "\n",
    "The very first state of the world, $s_0$, is randomly sampled from the **start-state distribution**, sometimes denoted by $\\rho_0$ :\n",
    "\n",
    "$$s_0 \\sim \\rho_0(\\cdot).$$\n",
    "\n",
    "State transitions (what happens to the world between the state at time $t$, $s_t$, and the state at $t+1$, $s_{t+1}$), are governed by the natural laws of the environment, and depend on only the most recent action, $a_t$. They can be either deterministic,\n",
    "\n",
    "$$s_{t+1} = f(s_t, a_t)$$\n",
    "\n",
    "or stochastic,\n",
    "\n",
    "$$s_{t+1} \\sim P(\\cdot|s_t, a_t).$$\n",
    "\n",
    "Actions come from an agent according to its policy.\n",
    "\n",
    "\n",
    "\n",
    "### Reward and Return\n",
    "\n",
    "The reward function $R$ is critically important in reinforcement learning. It depends on the current state of the world, the action just taken, and the next state of the world:\n",
    "\n",
    "$$r_t = R(s_t, a_t, s_{t+1})$$\n",
    "\n",
    "although frequently this is simplified to just a dependence on the current state, $r_t = R(s_t)$, or state-action pair $r_t = R(s_t,a_t)$.\n",
    "\n",
    "The goal of the agent is to maximize some notion of cumulative reward over a trajectory, but this actually can mean a few things. We'll notate all of these cases with $R(\\tau)$, and it will either be clear from context which case we mean, or it won't matter (because the same equations will apply to all cases).\n",
    "\n",
    "One kind of return is the **finite-horizon undiscounted return**, which is just the sum of rewards obtained in a fixed window of steps:\n",
    "\n",
    "$$R(\\tau) = \\sum_{t=0}^T r_t.$$\n",
    "\n",
    "Another kind of return is the **infinite-horizon discounted return**, which is the sum of all rewards *ever* obtained by the agent, but discounted by how far off in the future they're obtained. This formulation of reward includes a discount factor $\\gamma \\in (0,1)$ :\n",
    "\n",
    "$$R(\\tau) = \\sum_{t=0}^{\\infty} \\gamma^t r_t.$$\n",
    "\n",
    "Why would we ever want a discount factor, though? Don't we just want to get *all* rewards? We do, but the discount factor is both intuitively appealing and mathematically convenient. On an intuitive level: cash now is better than cash later. Mathematically: an infinite-horizon sum of rewards [may not converge]() to a finite value, and is hard to deal with in equations. But with a discount factor and under reasonable conditions, the infinite sum converges.\n",
    "\n",
    "\n",
    "### The RL Problem\n",
    "\n",
    "Whatever the choice of return measure (whether infinite-horizon discounted, or finite-horizon undiscounted), and whatever the choice of policy, the goal in RL is to select a policy which maximizes **expected return** when the agent acts according to it.\n",
    "\n",
    "To talk about expected return, we first have to talk about probability distributions over trajectories.\n",
    "\n",
    "Let's suppose that both the environment transitions and the policy are stochastic. In this case, the probability of a $T$ -step trajectory is:\n",
    "\n",
    "$$P(\\tau|\\pi) = \\rho_0 (s_0) \\prod_{t=0}^{T-1} P(s_{t+1} | s_t, a_t) \\pi(a_t | s_t).$$\n",
    "\n",
    "The expected return (for whichever measure), denoted by $J(\\pi)$, is then:\n",
    "\n",
    "$$J(\\pi) = \\int_{\\tau} P(\\tau|\\pi) R(\\tau) = E_{\\tau\\sim \\pi}{R(\\tau)}.$$\n",
    "\n",
    "\n",
    "\n",
    "The central optimization problem in RL can then be expressed by\n",
    "\n",
    "$$\\pi^* = \\arg \\max_{\\pi} J(\\pi),$$\n",
    "\n",
    "with $\\pi^*$ being the **optimal policy**.\n",
    "\n",
    "### Value Functions\n",
    "\n",
    "It's often useful to know the **value** of a state, or state-action pair. By value, we mean the expected return if you start in that state or state-action pair, and then act according to a particular policy forever after. **Value functions** are used, one way or another, in almost every RL algorithm.\n",
    "\n",
    "There are four main functions of note here.\n",
    "\n",
    "1.  The **On-Policy Value Function**, $V^{\\pi}(s)$, which gives the expected return if you start in state $s$ and always act according to policy $\\pi$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$V^{\\pi}(s) = E_{\\tau \\sim \\pi}{R(\\tau)\\left| s_0 = s\\right.}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  The **On-Policy Action-Value Function**, $Q^{\\pi}(s,a)$, which gives the expected return if you start in state $s$, take an arbitrary action $a$ (which may not have come from the policy), and then forever after act according to policy $\\pi$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$Q^{\\pi}(s,a) = E_{\\tau \\sim \\pi}{R(\\tau)\\left| s_0 = s, a_0 = a\\right.}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  The **Optimal Value Function**, $V^*(s)$, which gives the expected return if you start in state $s$ and always act according to the *optimal* policy in the environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$V^*(s) = \\max_{\\pi} E_{\\tau \\sim \\pi}{R(\\tau)\\left| s_0 = s\\right.}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.  The **Optimal Action-Value Function**, $Q^*(s,a)$, which gives the expected return if you start in state $s$, take an arbitrary action $a$, and then forever after act according to the *optimal* policy in the environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q^*(s,a) = \\max_{\\pi} E_{\\tau \\sim \\pi}{R(\\tau)\\left| s_0 = s, a_0 = a\\right.}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You Should Know\n",
    "\n",
    "There are two key connections between the value function and the action-value function that come up pretty often:\n",
    "\n",
    "$$V^{\\pi}(s) = E_{a\\sim \\pi}{Q^{\\pi}(s,a)},$$\n",
    "\n",
    "and\n",
    "\n",
    "$$V^*(s) = \\max_a Q^* (s,a).$$\n",
    "\n",
    "These relations follow pretty directly from the definitions just given: can you prove them?\n",
    "\n",
    "### The Optimal Q-Function and the Optimal Action\n",
    "\n",
    "There is an important connection between the optimal action-value function $Q^*(s,a)$ and the action selected by the optimal policy. By definition, $Q^*(s,a)$ gives the expected return for starting in state $s$, taking (arbitrary) action $a$, and then acting according to the optimal policy forever after.\n",
    "\n",
    "The optimal policy in $s$ will select whichever action maximizes the expected return from starting in $s$. As a result, if we have $Q^*$, we can directly obtain the optimal action, $a^*(s)$, via\n",
    "\n",
    "$$a^*(s) = \\arg \\max_a Q^* (s,a).$$\n",
    "\n",
    "Note: there may be multiple actions which maximize $Q^*(s,a)$, in which case, all of them are optimal, and the optimal policy may randomly select any of them. But there is always an optimal policy which deterministically selects an action.\n",
    "\n",
    "### Bellman Equations\n",
    "\n",
    "All four of the value functions obey special self-consistency equations called **Bellman equations**. The basic idea behind the Bellman equations is this:\n",
    "\n",
    "> The value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next.\n",
    "\n",
    "The Bellman equations for the on-policy value functions are\n",
    "\n",
    "$$\\begin{align*}\n",
    "V^{\\pi}(s) = E_{a \\sim \\pi \\\\ s'\\sim P}{r(s,a) + \\gamma V^{\\pi}(s')}, \\\\\n",
    "Q^{\\pi}(s,a) = E_{s'\\sim P}{r(s,a) + \\gamma E_{a'\\sim \\pi}{Q^{\\pi}(s',a')}},\n",
    "\\end{align*}$$\n",
    "\n",
    "where $s' \\sim P$ is shorthand for $s' \\sim P(\\cdot |s,a)$, indicating that the next state $s'$ is sampled from the environment's transition rules; $a \\sim \\pi$ is shorthand for $a \\sim \\pi(\\cdot|s)$; and $a' \\sim \\pi$ is shorthand for $a' \\sim \\pi(\\cdot|s')$.\n",
    "\n",
    "The Bellman equations for the optimal value functions are\n",
    "\n",
    "$$\\begin{align*}\n",
    "V^*(s) &= \\max_a E_{s'\\sim P}{r(s,a) + \\gamma V^*(s')}, \\\\\n",
    "Q^*(s,a) &= E_{s'\\sim P}{r(s,a) + \\gamma \\max_{a'} Q^*(s',a')}.\n",
    "\\end{align*}$$\n",
    "\n",
    "The crucial difference between the Bellman equations for the on-policy value functions and the optimal value functions, is the absence or presence of the $\\max$ over actions. Its inclusion reflects the fact that whenever the agent gets to choose its action, in order to act optimally, it has to pick whichever action leads to the highest value.\n",
    "\n",
    "You Should Know\n",
    "\n",
    "The term \"Bellman backup\" comes up quite frequently in the RL literature. The Bellman backup for a state, or state-action pair, is the right-hand side of the Bellman equation: the reward-plus-next-value.\n",
    "\n",
    "### Advantage Functions\n",
    "\n",
    "Sometimes in RL, we don't need to describe how good an action is in an absolute sense, but only how much better it is than others on average. That is to say, we want to know the relative **advantage** of that action. We make this concept precise with the **advantage function.**\n",
    "\n",
    "The advantage function $A^{\\pi}(s,a)$ corresponding to a policy $\\pi$ describes how much better it is to take a specific action $a$ in state $s$, over randomly selecting an action according to $\\pi(\\cdot|s)$, assuming you act according to $\\pi$ forever after. Mathematically, the advantage function is defined by\n",
    "\n",
    "$$A^{\\pi}(s,a) = Q^{\\pi}(s,a) - V^{\\pi}(s).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-armed bandit\n",
    "\n",
    "The simplest reinforcement learning problem is the multi-armed bandit. Essentially, there are $n$-many slot machines, each with a different fixed payout probability. \n",
    "\n",
    "The goal is to discover the machine with the best payout, and maximize the returned reward by always choosing it. \n",
    "\n",
    "<img src=\"images/slot.jpg\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "This question has been the subject of active research since the 1950s, and many variations have been studied.\n",
    "\n",
    "#### A-B Testing\n",
    "\n",
    "Traditional A-B testing can be thought of as a special case of the multi-armed bandit problem, in which we choose to pursue a strategy of pure exploration in the initial testing phase, followed by a period of pure exploitation in which we choose the most valuable “arm” 100% of the time.\n",
    "\n",
    "If the exploitation phase can be assumed to be much longer than the exploration phase, this approach is usually reasonable, as the wasted resources during the exploration are insignificant relative to the total rewards. However, in cases where the cost of the exploration phase is non-negligible, or in cases in which arm values are changing dynamically on short enough timescales that it becomes impractical to repeatedly perform new A-B tests, alternative approaches are needed.\n",
    "\n",
    "The n-armed bandit is a nice starting place because we don’t have to worry about aspects #2 and #3. All we need to focus on is learning which rewards we get for each of the possible actions, and ensuring we chose the optimal ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon-greedy\n",
    "\n",
    "The most straightforward algorithm for continuously balancing exploration with exploitation is called “epsilon-greedy”. \n",
    "\n",
    "Here, we pull a randomly chosen arm a fraction $\\epsilon$ of the time. The other $1-\\epsilon$ of the time, we pull the arm which we estimate to be the most profitable. As each arm is pulled and rewards are received, our estimates of arm values are updated. \n",
    "\n",
    "This method can be thought of a a continuous testing setup, where we devote a fraction $\\epsilon$ of our resources to testing.\n",
    "\n",
    "The following python code implements a simple 10-Armed Bandit using the epsilon-greedy algorithm. \n",
    "\n",
    "The payout rate of the arms are normally distributed with $\\mu = 0$ and $\\sigma = 1$. Gaussian noise is also added to the rewards, also with $\\mu = 0$ and $\\sigma = 1$. (See [Sutton and Barto](http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf) book, section 2.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although ε-greedy action selection is an effective and popular means of balancing exploration and exploitation in reinforcement learning, one drawback is that when it explores **it chooses equally among all actions**. \n",
    "\n",
    "This means that it is as likely to choose the worst-appearing action as it is to choose the next-to-best action. In tasks where the worst actions are very bad, this may\n",
    "be unsatisfactory. The obvious solution is to vary the action probabilities as a graded function of estimated value.\n",
    "\n",
    "The greedy action will be given the highest selection probability, but all the others will be ranked and weighted according to\n",
    "their value estimates. To this end we can use a softmax action selection rule (with a temperature parameter $\\tau$):\n",
    "\n",
    "$$\n",
    "\\frac{ e^{Q_t(a)/ \\tau}}{\\sum_{i=1}^n e^{Q_t(i)/ \\tau}}\n",
    "$$\n",
    "\n",
    "For high temperatures ( $\\tau \\to \\infty $ ), all actions have nearly the same probability. For a low temperature ( $ \\tau \\to 0^{+} $), the probability of the action with the highest expected reward tends to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import seaborn \n",
    "from tqdm import tqdm\n",
    "\n",
    "class Bandit: \n",
    "    def __init__(self): \n",
    "        self.arm_values = np.random.normal(0,1,10) \n",
    "        self.K = np.zeros(10) \n",
    "        self.est_values = np.zeros(10) \n",
    "\n",
    "    def get_reward(self,action): \n",
    "        noise = np.random.normal(0,1) \n",
    "        reward = self.arm_values[action] + noise \n",
    "        return reward \n",
    "\n",
    "    def choose_eps_greedy(self,epsilon):\n",
    "        rand_num = np.random.random() \n",
    "        if epsilon>rand_num: \n",
    "            return np.random.randint(10) \n",
    "        else: \n",
    "            return np.argmax(self.est_values)\n",
    "\n",
    "    def update_est(self,action,reward): \n",
    "        self.K[action] += 1 \n",
    "        alpha = 1./self.K[action] \n",
    "        self.est_values[action] += alpha * (reward - self.est_values[action]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are we estimating the value of an action?\n",
    "\n",
    "If by the $t$-th time step action $a$ has been chosen $K_a$ times prior to $t$, yielding rewards \n",
    "$R_1, R_2, . . . , R_{Ka}$, then its value is estimated to be:\n",
    "\n",
    "$$\n",
    "Q_t(a) = \\frac{R_1 + R_2 + . . .+ R_{Ka}}{K_a}\n",
    "$$\n",
    "\n",
    "A problem with this straightforward implementation is that its memory and computational requirements grow over time without\n",
    "bound (we have to maintain, for each action $a$, a record of all the rewards that have followed the\n",
    "selection of that action), but we can derive an incremental formula for computing averages with small, constant computation\n",
    "required to process each new reward.\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "Q_{k+1} & = & \\frac{1}{k+1} \\sum_{i=1}^{k+1} R_i \\\\\n",
    "  & = & \\frac{1}{k+1} \\left( R_{k+1} + \\sum_{i=1}^k R_i\n",
    "\\right) \\\\\n",
    "  & = & \\frac{1}{k+1} \\left( R_{k+1} + kQ_k + Q_k - Q_k\n",
    "\\right) \\\\\n",
    "  & = & \\frac{1}{k+1} \\left(R_{k+1} + (k+1) Q_k - Q_k\n",
    "\\right) \\\\\n",
    "  & = & Q_k + \\frac{1}{k+1} \\left( R_{k+1} - Q_k \\right)\n",
    "\\end{eqnarray*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(bandit,Npulls,epsilon):\n",
    "    history = [] \n",
    "    for i in range(Npulls): \n",
    "        action = bandit.choose_eps_greedy(epsilon)\n",
    "        R = bandit.get_reward(action) \n",
    "        bandit.update_est(action,R) \n",
    "        history.append(R) \n",
    "    return np.array(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make three different experiments: $\\epsilon = 0$, $\\epsilon = 0.1$ and $\\epsilon = 0.01$. Data will be averages over 2000 tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:14<00:00,  9.78it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Nexp = 2000\n",
    "Npulls = 5000 \n",
    "avg_outcome_eps0p0 = np.zeros(Npulls) \n",
    "avg_outcome_eps0p01 = np.zeros(Npulls) \n",
    "avg_outcome_eps0p1 = np.zeros(Npulls) \n",
    "\n",
    "for i in tqdm(range(Nexp)): \n",
    "    bandit = Bandit() \n",
    "    avg_outcome_eps0p0 += experiment(bandit,Npulls,0.0) \n",
    "    bandit = Bandit() \n",
    "    avg_outcome_eps0p01 += experiment(bandit,Npulls,0.01) \n",
    "    bandit = Bandit() \n",
    "    avg_outcome_eps0p1 += experiment(bandit,Npulls,0.1) \n",
    "\n",
    "avg_outcome_eps0p0 /= np.float(Nexp) \n",
    "avg_outcome_eps0p01 /= np.float(Nexp) \n",
    "avg_outcome_eps0p1 /= np.float(Nexp) \n",
    "\n",
    "# plot results \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(avg_outcome_eps0p0,label=\"eps = 0.0\", alpha=0.5) \n",
    "plt.plot(avg_outcome_eps0p01,label=\"eps = 0.01\", alpha=0.5) \n",
    "plt.plot(avg_outcome_eps0p1,label=\"eps = 0.1\", alpha=0.5) \n",
    "plt.ylim(0,2.2) \n",
    "plt.legend() \n",
    "plt.gcf().set_size_inches((8,3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "Unlike other methods, which attempt to learn functions which directly map an observation to an action, Q-Learning attempts **to learn the value of being in a given state, and taking a specific action there**. \n",
    "\n",
    "The agent is in a state $s$ and has to choose one action $a$, upon which it receives a reward $r$ and come to a new state $s’$. The way the agent chooses actions is called **policy**.\n",
    "\n",
    "Let’s define a function $Q(s, a)$ such that for given state $s$ and action $a$ it returns an estimate of a total reward we would achieve starting at this state, taking the action and then following some policy. Under certain conditions, there certainly exist policies that are optimal, meaning that they always select an action which is the best in the context. Let’s call the $Q$ function for these optimal policies $Q^*$.\n",
    "\n",
    "If we knew the true $Q^*$ function, the solution would be straightforward. We would just apply a greedy policy to it. That means that in each state $s$, we would just choose an action $a$ that maximizes the function $Q^*$, $argmax_a Q^*(s, a)$. Knowing this, our problem reduces to find a good estimate of the $Q^*$ function and apply the greedy policy to it.\n",
    "\n",
    "Let’s write a formula for this function in a symbolic way. It is a sum of rewards we achieve after each action, but we will discount every member with γ:\n",
    "\n",
    " $$ Q^*(s, a) = r_0 + \\gamma r_1 + \\gamma^2 r_2 + \\gamma^3 r_3 + ... $$\n",
    "\n",
    "$\\gamma$ is called a discount factor and when set it to $\\gamma < 1$ , it makes sure that the sum in the formula is finite. Value of each member exponentially diminish as they are more and more in the future and become zero in the limit. The $\\gamma$ therefore controls how much the function $Q$ in state $s$ depends on the future and so it can be thought of as how much ahead the agent sees. \n",
    "\n",
    "Typically we set it to a value close, but lesser to one. The actions are chosen according to the greedy policy, maximizing the $Q^*$ function.\n",
    "\n",
    "When we look again at the formula, we see that we can write it in a recursive form:\n",
    "\n",
    " $$Q^*(s, a) = r_0 + \\gamma (r_1 + \\gamma r_2 + \\gamma^2 r_3 + ...) = r_0 + \\gamma max_a Q^*(s', a)$$\n",
    " \n",
    "We just derived a so called **Bellman equation**.\n",
    " \n",
    "One of the possible strategies to solve the Bellman equation is by applying the **Q-learning** algorithm:\n",
    "\n",
    "```\n",
    "For each state-action pair (s, a), initialize the table entry Q(s,a) to zero\n",
    "Observe the current state s\n",
    "Do forever:\n",
    "- Select an action a from s and execute it \n",
    "- Receive immediate reward r\n",
    "- Observe the new state s'\n",
    "- Update the table entry for Q\n",
    "- s=s'\n",
    "```\n",
    "#### Action selection\n",
    "\n",
    "We could apply different strategies for action selection:\n",
    "+ **Random approach**. Only in circumstances where a random policy is optimal would this approach be ideal.\n",
    "+ **$\\epsilon$- greedy approach**:  A simple combination of the greedy and random approaches yields one of the most used exploration strategies. At the start of the training process the $\\epsilon$ value is often initialized to a large probability, to encourage exploration in the face of knowing little about the environment. The value is then annealed down to a small constant (often 0.1), as the agent is assumed to learn most of what it needs about the environment. Despite the prevalence of usage that it enjoys, this method is far from optimal, since it takes into account only whether actions are most rewarding or not.\n",
    "+ **Boltzmann Approach**. Instead of always taking the optimal action, or taking a random action, this approach involves choosing an action with weighted probabilities. To accomplish this we use a softmax over the networks estimates of value for each action.  In practice we utilize an additional temperature parameter ($\\tau$) which is annealed over time. While this measure can be a useful proxy, it is not exactly what would best aid exploration. What we really want to understand is the agent’s uncertainty about the value of different actions.\n",
    "+ **Bayesian Approaches**. What if an agent could exploit its own uncertainty about its actions? This is exactly the ability that a class of neural network models referred to as Bayesian Neural Networks (BNNs) provide. Unlike traditional neural network which act deterministically, BNNs act probabilistically. This means that instead of having a single set of fixed weights, a BNN maintains a probability distribution over possible weights. In a reinforcement learning setting, the distribution over weight values allows us to obtain distributions over actions as well. The variance of this distribution provides us an estimate of the agent’s uncertainty about each action. In order to get true uncertainty estimates, multiple samples are required, thus increasing computational complexity.\n",
    "\n",
    "\n",
    "#### Table updating\n",
    "\n",
    "The table entry for $Q$ is updated by using this formula:\n",
    "\n",
    "$$\n",
    "Q(s,a) = Q(s,a) + \\alpha [r + \\gamma max_{a'} Q(s',a') - Q(s,a) ]\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "+ $0<\\alpha<1$ is the learning rate. Setting it to 0 means that the Q-values are never updated, hence nothing is learned. Setting a high value such as 0.9 means that learning can occur quickly.\n",
    "+ $0<\\gamma<1$ is the discount factor. This models the fact that future rewards are worth less than immediate rewards. \n",
    "\n",
    "See http://www.scholarpedia.org/article/Temporal_difference_learning for a short description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the FrozenLake problem\n",
    "\n",
    "We are going to to solve the FrozenLake environment from the OpenAI gym. \n",
    "\n",
    "OpenAI gym (https://gym.openai.com/) provides an easy way for people to experiment with their learning agents in an array of provided toy games. The FrozenLake environment consists of a 4x4 grid of blocks, each one either being the start block, the goal block, a safe frozen block, or a dangerous hole. \n",
    "\n",
    "> **FrozenLake-v0**\n",
    "\n",
    "> The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
    "\n",
    "> The surface is described using a grid like the following:\n",
    "\n",
    ">``SFFF       (S: starting point, safe)``\n",
    "\n",
    ">``FHFH       (F: frozen surface, safe)``\n",
    "\n",
    ">``FFFH       (H: hole, fall to your doom)``\n",
    "\n",
    ">``HFFG       (G: goal, where the frisbee is located)``\n",
    "\n",
    "> The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n",
    "The objective is to have an agent learn to navigate from the start to the goal without moving onto a hole. At any given time the agent can choose to move either ``up, down, left``, or ``right``. \n",
    "\n",
    "The catch is that there is a wind which occasionally blows the agent onto a space they didn’t choose. As such, perfect performance every time is impossible, but learning to avoid the holes and reach the goal are certainly still doable. The reward at every step is 0, except for entering the goal, which provides a reward of 1. Thus, we will need an algorithm that learns long-term expected rewards. This is exactly what Q-Learning is designed to provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In it’s simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. Within each cell of the table, we learn a value for how good it is to take a given action within a given state. \n",
    "\n",
    "In the case of the FrozenLake environment, we have 16 possible states (one for each block), and 4 possible actions (the four directions of movement), giving us a $16 \\times 4$ table of Q-values. We start by initializing the table to be uniform (all zeros), and then as we observe the rewards we obtain for various actions, we update the table accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "\n",
    "# Set learning parameters\n",
    "lr = .9\n",
    "gamma = 0.95\n",
    "num_episodes = 10000\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "rList = []\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    \n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 999999:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + lr*(r + gamma*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    rList.append(rAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FrozenLake-v0 is considered \"solved\" when the agent obtains an average reward of at least 0.78 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.56\n",
      "Final Q-Table Values\n",
      "[[2.39278400e-01 1.03695832e-03 1.09679898e-03 1.03375717e-03]\n",
      " [1.11555916e-05 1.25546279e-05 1.18256725e-04 2.97833059e-01]\n",
      " [8.58555556e-04 1.10888860e-04 3.24661929e-04 2.13220715e-01]\n",
      " [1.13533416e-05 7.68274654e-04 1.36421941e-05 3.33458488e-02]\n",
      " [1.52376885e-01 3.07694138e-05 1.35127572e-05 1.27524297e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.04404437e-04 5.44203580e-05 9.15082942e-05 5.03739627e-07]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.75596538e-06 4.43157862e-05 6.02790258e-05 6.97470053e-01]\n",
      " [3.82319572e-05 2.90784559e-01 5.93252462e-05 1.20300519e-05]\n",
      " [6.54414096e-03 9.96999673e-07 8.71046068e-05 2.05422670e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 7.43340473e-02 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 9.05527611e-01 1.06416500e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Score over time: \" +  str(sum(rList[-100:])/100))\n",
    "print(\"Final Q-Table Values\")\n",
    "print(Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
