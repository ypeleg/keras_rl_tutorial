{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Method\n",
    "\n",
    "How do we solve  for the policy optimization problem which is to **maximize** the total reward given some parametrized policy? \n",
    "\n",
    "## Discounted future reward\n",
    "\n",
    "To begin with, for an episode the total reward is the sum of all the rewards. If our environment is stochastic, we can never be sure if we will get the same rewards the next time we perform the same actions. Thus the more we go into the future the more the total future reward may diverge. So for that reason it is common to use the **discounted future reward** where the parameter `discount` is called the discount factor and is between 0 and 1. \n",
    "\n",
    "A good strategy for an agent would be to always choose an action that maximizes the (discounted) future reward. In other words we want to maximize the expected reward per episode.\n",
    "\n",
    "## Parametrized policy\n",
    "\n",
    "A stochastic policy is defined as a conditional probability of some action given a state. A family of policies indexed by a parameter vector `theta` are called parametrized policies. These policies are defined analogous to the supervised learning classification or regression problems. In the case of discrete policies we output a vector of probabilities of the possible actions and in the case of continuous policies we output a mean and diagonal covariance of a Gaussian distribution from which we can then sample our continous actions.\n",
    "\n",
    "## Cross entropy method (CEM)\n",
    "\n",
    "So how do we solve for the policy optimization problem of maximizing the total (discounted) reward given some parametrized policy? The simplest approach is the derivative free optimization (DFO) which looks at this problem as a black box with respect to the parameter `theta`. We try out many different `theta` and store the rewards for each episode. The main idea then is to move towards good `theta`.\n",
    "\n",
    "One particular DFO approach is called the CEM. Here at any point in time, you maintain a distribution over parameter vectors and move the distribution towards parameters with higher reward. This works surprisingly well, even if its not that effictive when `theta` is a high dimensional vector.\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "The idea is to initialize the `mean` and `sigma` of a Gaussian and then for `n_iter` times we:\n",
    "\n",
    "1. collect `batch_size` samples of `theta` from a Gaussian with the current `mean` and `sigma`\n",
    "2. perform a noisy evaluation to get the total rewards with these `theta`s \n",
    "3. select `n_elite` of the best `theta`s into an elite set\n",
    "4. upate our `mean` and `sigma` to be that from the elite set\n",
    "\n",
    "## Discrete linear policy\n",
    "\n",
    "For the `CartPole-v0` case let us define the linear parametrized policy as the following diagram:\n",
    "\n",
    "```\n",
    "         │               ┌───theta ~ N(mean,std)───┐\n",
    "         │\n",
    "   4 observations        [[ 2.2  4.5 ]\n",
    "[-0.1 -0.4  0.06  0.5] *  [ 3.4  0.2 ]  + [[ 0.2 ]\n",
    "         |                [ 4.2  3.4 ]     [ 1.1 ]]\n",
    "         │                [ 0.1  9.0 ]]\n",
    "         |                     W              b\n",
    "    ┌────o────┐\n",
    "<─0─│2 actions│─1─>    = [-0.4  0.1] ──argmax()─> 1\n",
    "    └─o─────o─┘\n",
    "```\n",
    "\n",
    "Which means we can use the `Space` introspection of the `env` to create an appropriatly sized `theta` parameter vector from which we can use a part as the matrix `W` and the rest as the bias vector `b` so that the number of output probabilities correspond to the number of actions of our particular `env`.\n",
    "\n",
    "## Extra noise\n",
    "\n",
    "We can also add extra decayed noise to our distribution in the form of `extra_cov` which decays after `extra_decay_time` iterations.\n",
    "\n",
    "## Discounted total reward\n",
    "\n",
    "We can also return the discounted total reward per episode via the `discount` parameter of the `do_episode` function:\n",
    "\n",
    "```python\n",
    "...\n",
    "for t in xrange(num_steps):\n",
    "  ...\n",
    "  disc_total_rew += reward * discount**t\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Input, Model\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.memory import EpisodeParameterMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the environment and extract the number of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "obs_dim = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1 : Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/user/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1, 4)              0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(1,) + env.observation_space.shape)\n",
    "x = Flatten()(inp)\n",
    "x = Dense(nb_actions)(x)\n",
    "x = Activation('softmax')(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: deep network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1, 4)              0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 658\n",
      "Trainable params: 658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(1,) + env.observation_space.shape)\n",
    "x = Flatten()(inp)\n",
    "x = Dense(16)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(16)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(16)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(nb_actions)(x)\n",
    "x = Activation('softmax')(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we configure and compile our agent. You can use every built-in Keras optimizer and even the metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = EpisodeParameterMemory(limit=1000, window_length=1)\n",
    "cem = CEMAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
    "               batch_size=50, nb_steps_warmup=2000, train_interval=50, elite_frac=0.05)\n",
    "cem.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. You can always safely abort the training prematurely using Ctrl + C. cem.fit(env, nb_steps=100000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000 steps ...\n",
      "  21/1000: episode: 1, duration: 0.116s, episode steps: 21, steps per second: 180, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.060 [-1.001, 1.740], mean_best_reward: --\n",
      "  45/1000: episode: 2, duration: 0.014s, episode steps: 24, steps per second: 1736, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.127 [-0.944, 1.877], mean_best_reward: --\n",
      "  60/1000: episode: 3, duration: 0.011s, episode steps: 15, steps per second: 1359, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.073 [-1.970, 1.230], mean_best_reward: --\n",
      "  71/1000: episode: 4, duration: 0.009s, episode steps: 11, steps per second: 1194, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.116 [-1.397, 2.283], mean_best_reward: --\n",
      "  84/1000: episode: 5, duration: 0.009s, episode steps: 13, steps per second: 1400, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.098 [-2.378, 1.414], mean_best_reward: --\n",
      "  98/1000: episode: 6, duration: 0.009s, episode steps: 14, steps per second: 1500, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.087 [-2.043, 1.190], mean_best_reward: --\n",
      " 114/1000: episode: 7, duration: 0.009s, episode steps: 16, steps per second: 1759, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.096 [-0.739, 1.219], mean_best_reward: --\n",
      " 132/1000: episode: 8, duration: 0.012s, episode steps: 18, steps per second: 1511, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.060 [-1.565, 2.376], mean_best_reward: --\n",
      " 150/1000: episode: 9, duration: 0.014s, episode steps: 18, steps per second: 1273, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.058 [-2.008, 1.226], mean_best_reward: --\n",
      " 162/1000: episode: 10, duration: 0.012s, episode steps: 12, steps per second: 966, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.099 [-1.198, 2.075], mean_best_reward: --\n",
      " 176/1000: episode: 11, duration: 0.018s, episode steps: 14, steps per second: 761, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.857 [0.000, 1.000], mean observation: -0.099 [-3.078, 1.927], mean_best_reward: --\n",
      " 194/1000: episode: 12, duration: 0.016s, episode steps: 18, steps per second: 1144, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.079 [-1.526, 2.523], mean_best_reward: --\n",
      " 209/1000: episode: 13, duration: 0.010s, episode steps: 15, steps per second: 1547, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.097 [-1.331, 2.210], mean_best_reward: --\n",
      " 218/1000: episode: 14, duration: 0.007s, episode steps: 9, steps per second: 1334, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-2.812, 1.778], mean_best_reward: --\n",
      " 239/1000: episode: 15, duration: 0.012s, episode steps: 21, steps per second: 1760, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.083 [-0.593, 0.962], mean_best_reward: --\n",
      " 260/1000: episode: 16, duration: 0.013s, episode steps: 21, steps per second: 1616, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.762 [0.000, 1.000], mean observation: -0.036 [-3.120, 2.147], mean_best_reward: --\n",
      " 275/1000: episode: 17, duration: 0.012s, episode steps: 15, steps per second: 1283, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.079 [-0.969, 1.705], mean_best_reward: --\n",
      " 300/1000: episode: 18, duration: 0.014s, episode steps: 25, steps per second: 1733, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.083 [-1.374, 0.596], mean_best_reward: --\n",
      " 319/1000: episode: 19, duration: 0.011s, episode steps: 19, steps per second: 1700, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.053 [-2.302, 1.400], mean_best_reward: --\n",
      " 386/1000: episode: 20, duration: 0.033s, episode steps: 67, steps per second: 2023, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.221 [-0.491, 1.277], mean_best_reward: --\n",
      " 422/1000: episode: 21, duration: 0.019s, episode steps: 36, steps per second: 1914, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.060 [-1.234, 1.170], mean_best_reward: --\n",
      " 435/1000: episode: 22, duration: 0.009s, episode steps: 13, steps per second: 1449, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.093 [-1.184, 1.815], mean_best_reward: --\n",
      " 448/1000: episode: 23, duration: 0.009s, episode steps: 13, steps per second: 1451, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.105 [-2.804, 1.798], mean_best_reward: --\n",
      " 471/1000: episode: 24, duration: 0.024s, episode steps: 23, steps per second: 945, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.052 [-1.823, 1.004], mean_best_reward: --\n",
      " 482/1000: episode: 25, duration: 0.013s, episode steps: 11, steps per second: 848, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.092 [-1.190, 1.857], mean_best_reward: --\n",
      " 501/1000: episode: 26, duration: 0.011s, episode steps: 19, steps per second: 1688, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.067 [-1.740, 1.187], mean_best_reward: --\n",
      " 511/1000: episode: 27, duration: 0.007s, episode steps: 10, steps per second: 1493, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.148 [-2.057, 1.170], mean_best_reward: --\n",
      " 521/1000: episode: 28, duration: 0.008s, episode steps: 10, steps per second: 1331, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.151 [-2.682, 1.711], mean_best_reward: --\n",
      " 555/1000: episode: 29, duration: 0.019s, episode steps: 34, steps per second: 1760, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: 0.097 [-0.953, 1.998], mean_best_reward: --\n",
      " 566/1000: episode: 30, duration: 0.008s, episode steps: 11, steps per second: 1316, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.123 [-1.133, 1.936], mean_best_reward: --\n",
      " 579/1000: episode: 31, duration: 0.008s, episode steps: 13, steps per second: 1547, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.098 [-1.405, 2.264], mean_best_reward: --\n",
      " 594/1000: episode: 32, duration: 0.009s, episode steps: 15, steps per second: 1714, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.084 [-1.423, 2.409], mean_best_reward: --\n",
      " 625/1000: episode: 33, duration: 0.016s, episode steps: 31, steps per second: 1997, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.645 [0.000, 1.000], mean observation: -0.032 [-2.807, 1.783], mean_best_reward: --\n",
      " 636/1000: episode: 34, duration: 0.007s, episode steps: 11, steps per second: 1467, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.108 [-2.758, 1.810], mean_best_reward: --\n",
      " 644/1000: episode: 35, duration: 0.007s, episode steps: 8, steps per second: 1094, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.590, 2.565], mean_best_reward: --\n",
      " 661/1000: episode: 36, duration: 0.014s, episode steps: 17, steps per second: 1239, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.099 [-1.298, 0.764], mean_best_reward: --\n",
      " 670/1000: episode: 37, duration: 0.006s, episode steps: 9, steps per second: 1406, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.124 [-1.939, 1.194], mean_best_reward: --\n",
      " 683/1000: episode: 38, duration: 0.011s, episode steps: 13, steps per second: 1171, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.107 [-2.778, 1.781], mean_best_reward: --\n",
      " 693/1000: episode: 39, duration: 0.009s, episode steps: 10, steps per second: 1162, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.132 [-1.762, 2.711], mean_best_reward: --\n",
      " 707/1000: episode: 40, duration: 0.009s, episode steps: 14, steps per second: 1615, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.083 [-0.998, 1.700], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 753/1000: episode: 41, duration: 0.031s, episode steps: 46, steps per second: 1474, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: 0.127 [-1.527, 1.528], mean_best_reward: --\n",
      " 765/1000: episode: 42, duration: 0.014s, episode steps: 12, steps per second: 865, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.093 [-1.227, 1.992], mean_best_reward: --\n",
      " 774/1000: episode: 43, duration: 0.011s, episode steps: 9, steps per second: 827, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.176 [-1.337, 2.359], mean_best_reward: --\n",
      " 784/1000: episode: 44, duration: 0.018s, episode steps: 10, steps per second: 565, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.139 [-1.526, 2.479], mean_best_reward: --\n",
      " 799/1000: episode: 45, duration: 0.025s, episode steps: 15, steps per second: 591, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.111 [-0.749, 1.397], mean_best_reward: --\n",
      " 816/1000: episode: 46, duration: 0.016s, episode steps: 17, steps per second: 1035, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.049 [-2.106, 1.405], mean_best_reward: --\n",
      " 834/1000: episode: 47, duration: 0.025s, episode steps: 18, steps per second: 714, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.053 [-1.493, 0.833], mean_best_reward: --\n",
      " 845/1000: episode: 48, duration: 0.017s, episode steps: 11, steps per second: 648, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.110 [-1.408, 2.309], mean_best_reward: --\n",
      " 861/1000: episode: 49, duration: 0.017s, episode steps: 16, steps per second: 927, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.083 [-1.473, 0.786], mean_best_reward: --\n",
      " 881/1000: episode: 50, duration: 0.017s, episode steps: 20, steps per second: 1146, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.045 [-1.006, 1.655], mean_best_reward: --\n",
      " 894/1000: episode: 51, duration: 0.013s, episode steps: 13, steps per second: 987, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.096 [-1.930, 1.211], mean_best_reward: --\n",
      " 904/1000: episode: 52, duration: 0.011s, episode steps: 10, steps per second: 927, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.125 [-2.553, 1.534], mean_best_reward: --\n",
      " 914/1000: episode: 53, duration: 0.014s, episode steps: 10, steps per second: 690, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [-1.994, 3.014], mean_best_reward: --\n",
      " 927/1000: episode: 54, duration: 0.021s, episode steps: 13, steps per second: 621, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.112 [-1.775, 0.983], mean_best_reward: --\n",
      " 938/1000: episode: 55, duration: 0.014s, episode steps: 11, steps per second: 772, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.123 [-1.547, 2.499], mean_best_reward: --\n",
      " 982/1000: episode: 56, duration: 0.036s, episode steps: 44, steps per second: 1228, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.568 [0.000, 1.000], mean observation: -0.114 [-2.376, 1.167], mean_best_reward: --\n",
      " 997/1000: episode: 57, duration: 0.015s, episode steps: 15, steps per second: 974, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.091 [-1.390, 2.326], mean_best_reward: --\n",
      "done, took 0.941 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4314154b10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cem.fit(env, nb_steps=1000, visualize=False, verbose=2)\n",
    "#cem.fit(env, nb_steps=100000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training is done, we save the best weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cem.save_weights('cem_{}_params.h5f'.format(ENV_NAME), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, evaluate our algorithm for 5 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 10.000, steps: 10\n",
      "Episode 2: reward: 10.000, steps: 10\n",
      "Episode 3: reward: 10.000, steps: 10\n",
      "Episode 4: reward: 10.000, steps: 10\n",
      "Episode 5: reward: 9.000, steps: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4314056710>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cem.test(env, nb_episodes=5, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
